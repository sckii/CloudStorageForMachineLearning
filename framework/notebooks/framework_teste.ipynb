{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (VotingClassifier, RandomForestClassifier)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import (precision_recall_curve, f1_score, fbeta_score, auc, make_scorer, confusion_matrix)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set()\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_arqs(pop):\n",
    "    PATH = \"../../eval/\"\n",
    "    name_space = []\n",
    "    arq = open(PATH + \"nsJanelas_\" + pop + \".txt\", \"r\")\n",
    "    arqPath = lambda name: f\"{PATH}/{name}_{pop}.txt\"\n",
    "\n",
    "    for line in arq:\n",
    "        name_space.append(sorted(list(map(int, line.split()))))\n",
    "\n",
    "    arq_acessos = pd.read_csv(arqPath(\"access\"), low_memory=False, sep=\" \", index_col=\"NameSpace\")\n",
    "    arq_classes = pd.read_csv(arqPath(\"target\"), low_memory=False, sep=\" \", index_col=\"NameSpace\")\n",
    "    arq_vol_bytes = pd.read_csv(arqPath(\"vol_bytes\"), low_memory=False, sep=\" \", index_col=\"NameSpace\")\n",
    "\n",
    "    return arq_acessos, arq_classes, arq_vol_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_acessos, arq_classes, arq_vol_bytes = read_arqs(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region constants\n",
    "HOT, WARM = [0, 1]\n",
    "COSTSTORAGEHOT, COSTSTORAGEWARM = [0.0230, 0.0125]\n",
    "COSTOPERATIONHOT, COSTOPERATIONWARM = [0.0004, 0.0010]\n",
    "COSTRETRIEVALHOT, COSTRETRIEVALWARM = [0.0000, 0.0100]\n",
    "# endregion\n",
    "\n",
    "def object_cost(vol_gb, acc, obj_class):\n",
    "    acc_1k = float(acc) / 1000.0  # acc prop to 1000\n",
    "    cost = 0\n",
    "    if obj_class == HOT:\n",
    "        cost = (vol_gb * COSTSTORAGEHOT + acc_1k * COSTOPERATIONHOT + vol_gb * acc * COSTRETRIEVALHOT)\n",
    "    else:  # warm\n",
    "        cost = (vol_gb * COSTSTORAGEWARM + acc_1k * COSTOPERATIONWARM + vol_gb * acc * COSTRETRIEVALWARM)\n",
    "    return cost\n",
    "\n",
    "\n",
    "def threshold_access(vol_gb, obj_class):\n",
    "    if obj_class == \"HW\":  # hot to warm\n",
    "        return int(vol_gb * (COSTSTORAGEWARM - COSTSTORAGEHOT) / ( COSTOPERATIONHOT - COSTOPERATIONWARM - vol_gb * 1000 * (COSTRETRIEVALWARM - COSTRETRIEVALHOT)))\n",
    "\n",
    "\n",
    "def get_optimal_cost(acc_fut, vol_gb, costs):\n",
    "    # Limiares de acesso para camadas H-W e W-C\n",
    "    acc_thres_hw = threshold_access(vol_gb, \"HW\")\n",
    "    QQ0 = QW0 = 0\n",
    "    if acc_fut > acc_thres_hw:  # HOT\n",
    "        costs[\"opt\"] += object_cost(vol_gb, acc_fut, HOT)\n",
    "        QQ0 += 1\n",
    "    else:  # WARM\n",
    "        costs[\"opt\"] += object_cost(vol_gb, acc_fut, WARM)\n",
    "        QW0 += 1\n",
    "\n",
    "\n",
    "def get_classifier_cost(row, costs, vol_gb, acc_fut, FPPenaltyEnabled):\n",
    "    if row[\"label\"] == 0 and row[\"pred\"] == 0:\n",
    "        # Se o objeto é warm e modelo acertou, adiciona custo de mudança pra warm\n",
    "        costs[\"TN\"] += object_cost(vol_gb, acc_fut, WARM)\n",
    "    elif row[\"label\"] == 0 and row[\"pred\"] == 1:\n",
    "        # Se o objeto é warm e modelo errou, adiciona penalidade por erro\n",
    "        # if FPPenaltyEnabled:\n",
    "        costs[\"FP\"] += object_cost(vol_gb, acc_fut, WARM)  # - penalty\n",
    "        costs[\"FP\"] += object_cost(vol_gb, acc_fut, HOT)  # - accesses in hot tier\n",
    "    elif row[\"label\"] == 1 and row[\"pred\"] == 0:\n",
    "        # Se o objeto é hot e modelo errou, adiciona penalidade por erro\n",
    "        costs[\"FN\"] += object_cost(vol_gb, 1, WARM)  # - one access to return to hot tier\n",
    "        # costs[\"FN\"] += object_cost(vol_gb, acc_fut - 1, Hot)  # - accesses in hot tier\n",
    "    elif row[\"label\"] == 1 and row[\"pred\"] == 1:\n",
    "        # Se o objeto é hot e modelo acertou, adiciona custo de permanencia em hot\n",
    "        costs[\"TP\"] += object_cost(vol_gb, acc_fut, HOT)\n",
    "\n",
    "\n",
    "def costs_of_all_classifiers(costs):\n",
    "    pred_cost = costs[\"TP\"] + costs[\"FN\"] + costs[\"TN\"] + costs[\"FP\"]  # Custo1\n",
    "    opt_cost = costs[\"opt\"]\n",
    "    default_cost = costs[\"always_H\"]  #: always hot\n",
    "    default_rcs = (default_cost - opt_cost) / default_cost\n",
    "    pred_rcs = (default_cost - pred_cost) / default_cost\n",
    "    return {\n",
    "        \"rcs ml\": pred_rcs,\n",
    "        \"rcs opt\": default_rcs,\n",
    "        \"cost ml\": pred_cost,\n",
    "        \"cost opt\": opt_cost,\n",
    "        \"cost all hot\": default_cost,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_costs(df, single_score=False, FPPenaltyEnabled=True):\n",
    "    costs = defaultdict(float)  # default = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        vol_gb = float(row[\"vol_bytes\"]) / (1024.0**3)  # vol per GB\n",
    "        acc_fut = row[\"acc_fut\"]\n",
    "\n",
    "        # Custo otimo\n",
    "        # costs vai por referencia, volta com os valores preenchidos nas colunas\n",
    "        get_optimal_cost(acc_fut, vol_gb, costs)\n",
    "\n",
    "        # Custo classificador\n",
    "        get_classifier_cost(row, costs, vol_gb, acc_fut, FPPenaltyEnabled)\n",
    "\n",
    "        # Custo simples sem otimização\n",
    "        costs[\"always_H\"] += object_cost(vol_gb, acc_fut, HOT)  #: always hot\n",
    "\n",
    "    pred_cost = costs[\"TP\"] + costs[\"FN\"] + costs[\"TN\"] + costs[\"FP\"]  # Custo1\n",
    "    if single_score:\n",
    "        return pred_cost\n",
    "    else:\n",
    "        return costs_of_all_classifiers(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_by_window(time_window, windowSize):\n",
    "    first_period_week = time_window\n",
    "    last_period_week = first_period_week + windowSize\n",
    "    return [first_period_week, last_period_week]\n",
    "\n",
    "def get_period_stamps(time_window, windowSize):\n",
    "    firstPeriod, lastPeriod = get_period_by_window(time_window, windowSize)\n",
    "    firstPeriodLAbel, lastPeriodLabel = get_period_by_window(lastPeriod, windowSize)\n",
    "    return [firstPeriod, lastPeriod, firstPeriodLAbel, lastPeriodLabel]\n",
    "\n",
    "def get_all_windows(time_window, windowSize, stepsToTake):\n",
    "    steps = stepsToTake * time_window\n",
    "\n",
    "    initialTrain, endTrain, initialTrainLabel, endTrainLabel = get_period_stamps(steps, windowSize)\n",
    "    (initialEvaluation, endEvaluation, initialEvaluationLabel, endEvaluationLabel) = get_period_stamps(initialTrainLabel, windowSize)\n",
    "    \n",
    "    endEvaluationLabel = initialEvaluationLabel + stepsToTake\n",
    "    \n",
    "    return [\n",
    "        initialTrain,\n",
    "        endTrain,\n",
    "        initialTrainLabel,\n",
    "        endTrainLabel,\n",
    "        initialEvaluation,\n",
    "        endEvaluation,\n",
    "        initialEvaluationLabel,\n",
    "        endEvaluationLabel,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_classifiers(probability=False, random_state=42):\n",
    "    default_clfs = {\n",
    "        \"SVMR\": svm.SVC(\n",
    "            gamma=\"auto\", probability=probability, random_state=random_state\n",
    "        ),  # rbf\n",
    "        \"SVML\": svm.SVC(\n",
    "            kernel=\"linear\", probability=probability, random_state=random_state\n",
    "        ),\n",
    "        \"SVMS\": svm.SVC(\n",
    "            kernel=\"sigmoid\", probability=probability, random_state=random_state\n",
    "        ),\n",
    "        \"RF\": RandomForestClassifier(n_jobs=-1, random_state=random_state),\n",
    "        \"KNN\": KNeighborsClassifier(n_jobs=-1),\n",
    "        \"DCT\": DecisionTreeClassifier(random_state=random_state),\n",
    "        \"LR\": LogisticRegression(n_jobs=-1),\n",
    "    }\n",
    "    return {\n",
    "        **default_clfs,\n",
    "        \"HV\": VotingClassifier(list(default_clfs.items()), voting=\"hard\", n_jobs=-1),\n",
    "        \"SV\": VotingClassifier(list(default_clfs.items()), voting=\"soft\", n_jobs=-1),\n",
    "    }\n",
    "\n",
    "\n",
    "def set_classifier(clf_key, clfs_dict):\n",
    "    clf = clfs_dict.get(clf_key, None)\n",
    "    if clf is None:\n",
    "        print(\"Unknown classifier!\")\n",
    "        sys.exit(1)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, window=0, rel=False):\n",
    "    y_true, y_pred = df[\"label\"].values, df[\"pred\"].values\n",
    "    # precision1, recall1, _ = precision_recall_curve(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    total_samples = len(y_true)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data={\n",
    "            \"Qtd obj\": total_samples,\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"auc_roc\": roc_auc_score(y_true, y_pred),\n",
    "            \"f1_score\": f1_score(y_true, y_pred),\n",
    "            \"f_beta_2\": fbeta_score(y_true, y_pred, beta=2),\n",
    "            #         \"auc\": auc(recall1, precision1),\n",
    "            \"Precision\": precision_score(y_true, y_pred),\n",
    "            \"Recall\": recall_score(y_true, y_pred),\n",
    "            \"P\": f\"{fn + tp} ({(fn + tp) / total_samples:.2%})\" if rel else (fn + tp),\n",
    "            \"N\": f\"{fp + tn} ({(fp + tn) / total_samples:.2%})\" if rel else fp + tn,\n",
    "            \"TP\": f\"{tp} ({tp / total_samples:.2%})\" if rel else tp,\n",
    "            \"FP\": f\"{fp} ({fp / total_samples:.2%})\" if rel else fp,\n",
    "            \"FN\": f\"{fn} ({fn / total_samples:.2%})\" if rel else fn,\n",
    "            \"TN\": f\"{tn} ({tn / total_samples:.2%})\" if rel else tn,\n",
    "        },\n",
    "        index=[window],\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_results(df, window, log_id, clf_name, FPPenaltyEnabled):\n",
    "    metrics = calculate_metrics(df)\n",
    "    costs = calculate_costs(df)\n",
    "\n",
    "    results = {\n",
    "        \"Dados\": log_id,\n",
    "        \"Model\": clf_name,\n",
    "        #                \"Time window\": window,\n",
    "        \"Qtd obj\": df.shape[0],\n",
    "        **metrics.to_dict(orient=\"list\"),\n",
    "        **costs,\n",
    "    }\n",
    "    return pd.DataFrame(results, index=[window])\n",
    "\n",
    "\n",
    "def aggregate_results(df_results, gp_objs=5, std=False):\n",
    "    df_mean = pd.DataFrame()\n",
    "\n",
    "    for i, df in enumerate(df_results[:gp_objs], start=1):\n",
    "        df = df.drop(columns=[\"P\", \"N\", \"Qtd obj\"])  # ,\"TP\", \"FP\", \"FN\", \"TN\"])\n",
    "        mean = df.mean()\n",
    "\n",
    "        if std:\n",
    "            df_std = df.std()\n",
    "            df_std.loc[[\"TP\", \"FP\", \"FN\", \"TN\"]] = np.nan\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                data=np.array(\n",
    "                    [\n",
    "                        f\"{x:.5f} ({y:.2%})\" if not np.isnan(y) else f\"{x:.2f}\"\n",
    "                        for x, y in zip(mean, df_std)\n",
    "                    ]\n",
    "                ).reshape(1, -1),\n",
    "                columns=df.columns,\n",
    "                index=[i],\n",
    "            )\n",
    "        else:\n",
    "            df = pd.DataFrame(\n",
    "                data=np.array([f\"{x:.5f}\" for x in mean]).reshape(1, -1),\n",
    "                index=[i],\n",
    "                columns=df.columns,\n",
    "            )\n",
    "\n",
    "        df_mean = pd.concat([df_mean, df]).loc[:, [*df.columns]]\n",
    "\n",
    "    return df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ML(initial_train, end_train, initial_train_label, end_train_label, resample, classifier_name, classifiers_dictionary):\n",
    "    # TRAINING\n",
    "    acc_train = arq_acessos.iloc[:, initial_train:end_train]\n",
    "\n",
    "    # filtrando volume 0 (objetos que ainda não existem nessa janela de tempo)\n",
    "    vol_train = arq_vol_bytes.iloc[:, end_train - 1]\n",
    "    idx = vol_train.values.ravel() > 0.0\n",
    "    acc_train = acc_train[idx]\n",
    "\n",
    "    # train_label = arq_classes.iloc[:, idx_train_label]\n",
    "    train_label = arq_classes.iloc[:, initial_train_label:end_train_label]  #\n",
    "    train_label = train_label.apply(lambda row: int(row.any()), axis=1)  # Aqui se existir algum 1 na janela de label de treino então pegamos 1, se não, 0.\n",
    "\n",
    "    train_label = train_label[idx]  # filtrando volume 0\n",
    "\n",
    "    # dealing with the imbalanced dataset\n",
    "    # X_train, y_train = resample.fit_resample(acc_train, train_label)\n",
    "    clf = set_classifier(classifier_name, classifiers_dictionary)\n",
    "    clf.fit(acc_train, train_label)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_training_parameters(arq_acessos, random_state=42, **kwargs):\n",
    "    num_weeks = arq_acessos.shape[1]\n",
    "    scaler = kwargs.get(\"scaler\", MinMaxScaler(feature_range=(0, 1)))\n",
    "    resample = kwargs.get(\"resample\", RandomUnderSampler(random_state=random_state))\n",
    "    clfs_dict = kwargs.get(\"clfs_dict\", get_default_classifiers(True, random_state))\n",
    "    \n",
    "    return [\n",
    "        pd.DataFrame(),\n",
    "        range((num_weeks // steps_to_take) - ((2 * window_size) // steps_to_take)),\n",
    "        scaler,\n",
    "        resample,\n",
    "        clfs_dict,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier_name, acc_eval, clf, initial_evaluation, end_evaluation):\n",
    "    if classifier_name != \"ONLINE\":\n",
    "        X_eval = acc_eval\n",
    "        y_hat_by_obj = clf.predict(X_eval)\n",
    "        y_hat_obj_red = y_hat_by_obj  # .apply(lambda row: int(row.any()), axis=1)\n",
    "    # calculando métricas\n",
    "    else:\n",
    "        y_hat_by_obj = arq_classes.iloc[:, initial_evaluation:end_evaluation]\n",
    "        y_hat_obj_red = y_hat_by_obj.apply(lambda row: int(row.any()), axis=1)\n",
    "    return y_hat_obj_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(initial_evaluation, end_evaluation, initial_evaluation_label, end_evaluation_label):\n",
    "    # EVALUATION\n",
    "    acc_eval = arq_acessos.iloc[:, initial_evaluation:end_evaluation]\n",
    "\n",
    "    # filtrando volume 0 (objetos que ainda não existem nessa janela de tempo )\n",
    "    vol_eval = arq_vol_bytes.iloc[:, end_evaluation - 1]\n",
    "    idx = vol_eval.values.ravel() > 0.0\n",
    "    acc_eval = acc_eval[idx]\n",
    "\n",
    "    # eval_label = arq_classes.iloc[:, idx_eval_label]\n",
    "    eval_label = arq_classes.iloc[:, initial_evaluation_label:end_evaluation_label]  #\n",
    "    eval_label = eval_label.apply(lambda row: int(row.any()), axis=1)  #\n",
    "\n",
    "    eval_label = eval_label[idx]  # filtrando volume 0\n",
    "    return acc_eval, eval_label  # o retorno são os dados de acesso e se há acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_eval(classifier_name, random_state, window_size, steps_to_take, pop, print_df=True, FP_penalty_enabled=True, **kwargs):\n",
    "\n",
    "    (df_metrics_eval, time_total, scaler, resample,classifiers_dictionary) = initialize_training_parameters(arq_acessos, random_state, **kwargs)\n",
    "\n",
    "    for time_window in time_total:\n",
    "\n",
    "        (initial_train, end_train, initial_train_label, end_train_label, initial_evaluation, end_evaluation, initial_evaluation_label, end_evaluation_label) = get_all_windows(time_window, window_size, steps_to_take)\n",
    "\n",
    "        if classifier_name != \"ONLINE\":\n",
    "            clf = train_ML( initial_train, end_train, initial_train_label, end_train_label, resample, classifier_name, classifiers_dictionary)\n",
    "\n",
    "        acc_eval, eval_label = evaluate(initial_evaluation, end_evaluation, initial_evaluation_label, end_evaluation_label)\n",
    "\n",
    "        if classifier_name != \"ONLINE\":\n",
    "            y_hat_obj_red = predict(classifier_name, acc_eval, clf, initial_evaluation, end_evaluation)\n",
    "        else:\n",
    "            y_hat_obj_red = predict(classifier_name, acc_eval, None, initial_evaluation, end_evaluation)\n",
    "\n",
    "        first_col, last_col = acc_eval.columns[0], acc_eval.columns[-1]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df_to_evaluate = pd.DataFrame()\n",
    "        df[\"label\"] = eval_label.squeeze().values\n",
    "        df[\"pred\"] = y_hat_obj_red\n",
    "\n",
    "        vol_eval = arq_vol_bytes.iloc[:, initial_evaluation_label:end_evaluation_label]\n",
    "        acc_eval = arq_acessos.iloc[:, initial_evaluation_label:end_evaluation_label]\n",
    "        df[\"total_vol\"] = vol_eval.sum(axis=1)\n",
    "\n",
    "        acc_with_no_zeroes = acc_eval[df.total_vol > 0]\n",
    "        vol_with_no_zeroes = vol_eval[df.total_vol > 0]\n",
    "        df_with_no_zeroes  = df[df.total_vol > 0]\n",
    "\n",
    "        print(df_with_no_zeroes)\n",
    "        \n",
    "        for i in range(steps_to_take):\n",
    "            df_to_evaluate[\"vol_bytes\"] = vol_with_no_zeroes.iloc[:, i]\n",
    "            df_to_evaluate[\"acc_fut\"] = acc_with_no_zeroes.iloc[:, i]\n",
    "            df_to_evaluate[\"label\"] = df_with_no_zeroes.label[:]\n",
    "            df_to_evaluate[\"pred\"] = df_with_no_zeroes.pred[:]\n",
    "            eval_metrics = generate_results(\n",
    "                df_to_evaluate,\n",
    "                f\"{first_col}:{last_col}\",\n",
    "                pop,\n",
    "                classifier_name,\n",
    "                FP_penalty_enabled,\n",
    "            )\n",
    "            df_metrics_eval = pd.concat([df_metrics_eval, eval_metrics])\n",
    "\n",
    "    return df_metrics_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pred\n",
      "0      1     1\n",
      "1      0     0\n",
      "2      1     1\n",
      "3      1     1\n",
      "   label  pred     total_vol\n",
      "0      1     1  4.294967e+09\n",
      "1      0     0  4.294967e+09\n",
      "2      1     1  4.294967e+09\n",
      "3      1     1  4.294967e+09\n",
      "   label  pred\n",
      "0      0     1\n",
      "1      0     0\n",
      "2      1     1\n",
      "3      1     1\n",
      "   label  pred     total_vol\n",
      "0      0     1  4.294967e+09\n",
      "1      0     0  4.294967e+09\n",
      "2      1     1  4.294967e+09\n",
      "3      1     1  4.294967e+09\n"
     ]
    }
   ],
   "source": [
    "pop = \"Pop4\"\n",
    "clf_name = \"LR\"\n",
    "window_size = 4\n",
    "steps_to_take = 4\n",
    "\n",
    "# agregar resultados? proximo parametro\n",
    "# habilitar grafico\n",
    "print_df = False\n",
    "# penalidade para falso positivo\n",
    "FP_penalty_enabled = True  # param de entrada\n",
    "\n",
    "arq_acessos, arq_classes, arq_vol_bytes = read_arqs(pop)\n",
    "costs = pd.DataFrame()\n",
    "results = {}\n",
    "random_state = 42\n",
    "progressionBarDescription = None\n",
    "\n",
    "costs_over = pd.DataFrame()\n",
    "results_over = {}\n",
    "costs_smote = pd.DataFrame()\n",
    "results_smote = {}\n",
    "\n",
    "results[clf_name] = run_train_eval(clf_name, random_state, window_size, steps_to_take, pop)\n",
    "costs[clf_name] = results[clf_name][[\"cost ml\", \"cost all hot\", \"cost opt\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    LR\n",
      "cost ml       0.702905\n",
      "cost all hot  0.736905\n",
      "cost opt      0.600405\n"
     ]
    }
   ],
   "source": [
    "print(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
